
[ Before Graph Optimisation ]
0     FullyConnected       StatefulPartitionedCall_1:0   
1     Relu                 StatefulPartitionedCall_1:0   


[ After Graph Optimization ]
0     FullyConnected       StatefulPartitionedCall_1:0   
1     Relu                 StatefulPartitionedCall_1:0   


[ Graph With Tensor Quantization ]
0 FullyConnected StatefulPartitionedCall_1:0
    Input 00 Int8 scale: [(scale:1077875712, shift:38)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 serving_default_input_1:0
    Input 01 Int8 scale: [(scale:1385161216, shift:40), (scale:1380709120, shift:40), (scale:1363363200, shift:40), (scale:1377983360, shift:40), (scale:1355704576, shift:40), (scale:1391378816, shift:40), (scale:1392280064, shift:40), (scale:1380292096, shift:40), (scale:1387211136, shift:40), (scale:1384919424, shift:40), (scale:1389811072, shift:40), (scale:1377946496, shift:40), (scale:1383744128, shift:40), (scale:1390694656, shift:40), (scale:1390958208, shift:40), (scale:1389075456, shift:40), (scale:1388239232, shift:40), (scale:1390788992, shift:40), (scale:1385021056, shift:40), (scale:1390101376, shift:40), (scale:1369987840, shift:40), (scale:1384091904, shift:40), (scale:1383907072, shift:40), (scale:1391177984, shift:40), (scale:1388157184, shift:40), (scale:1389058816, shift:40), (scale:1386805504, shift:40), (scale:1391771264, shift:40), (scale:1390870528, shift:40), (scale:1380812288, shift:40), (scale:1380664832, shift:40), (scale:1387885056, shift:40)], zero_point: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], quantMin: [], quantMax: [], dimension: 0 tfl.pseudo_qconst1
    Input 02 Int32 scale: [(scale:1390494080, shift:48), (scale:1386024832, shift:48), (scale:1368612096, shift:48), (scale:1383288576, shift:48), (scale:1360924032, shift:48), (scale:1396735616, shift:48), (scale:1397640320, shift:48), (scale:1385606144, shift:48), (scale:1392551936, shift:48), (scale:1390251392, shift:48), (scale:1395161856, shift:48), (scale:1383251584, shift:48), (scale:1389071488, shift:48), (scale:1396048768, shift:48), (scale:1396313344, shift:48), (scale:1394423424, shift:48), (scale:1393583872, shift:48), (scale:1396143488, shift:48), (scale:1390353408, shift:48), (scale:1395453184, shift:48), (scale:1375262208, shift:48), (scale:1389420672, shift:48), (scale:1389235072, shift:48), (scale:1396534016, shift:48), (scale:1393501568, shift:48), (scale:1394406656, shift:48), (scale:1392144640, shift:48), (scale:1397129600, shift:48), (scale:1396225408, shift:48), (scale:1386128384, shift:48), (scale:1385980416, shift:48), (scale:1393228416, shift:48)], zero_point: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], quantMin: [], quantMax: [], dimension: 0 tfl.pseudo_qconst
    Output 00 Int8 scale: [(scale:1387409664, shift:37)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 StatefulPartitionedCall_1:0
1 Relu StatefulPartitionedCall_1:0
    Input 00 Int8 scale: [(scale:1387409664, shift:37)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 StatefulPartitionedCall_1:0
    Output 00 Int8 scale: [(scale:1387409664, shift:37)], zero_point: [-128], quantMin: [-128], quantMax: [], dimension: 0 StatefulPartitionedCall_1:0


[ Before Graph Optimisation ]
0     FullyConnected       StatefulPartitionedCall_1:0   
1     Relu                 StatefulPartitionedCall_1:0   


[ After Graph Optimization ]
0     FullyConnected       StatefulPartitionedCall_1:0   
1     Relu                 StatefulPartitionedCall_1:0   


[ Graph With Tensor Quantization ]
0 FullyConnected StatefulPartitionedCall_1:0
    Input 00 Int8 scale: [(scale:1077875712, shift:38)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 serving_default_input_1:0
    Input 01 Int8 scale: [(scale:1385161216, shift:40), (scale:1380709120, shift:40), (scale:1363363200, shift:40), (scale:1377983360, shift:40), (scale:1355704576, shift:40), (scale:1391378816, shift:40), (scale:1392280064, shift:40), (scale:1380292096, shift:40), (scale:1387211136, shift:40), (scale:1384919424, shift:40), (scale:1389811072, shift:40), (scale:1377946496, shift:40), (scale:1383744128, shift:40), (scale:1390694656, shift:40), (scale:1390958208, shift:40), (scale:1389075456, shift:40), (scale:1388239232, shift:40), (scale:1390788992, shift:40), (scale:1385021056, shift:40), (scale:1390101376, shift:40), (scale:1369987840, shift:40), (scale:1384091904, shift:40), (scale:1383907072, shift:40), (scale:1391177984, shift:40), (scale:1388157184, shift:40), (scale:1389058816, shift:40), (scale:1386805504, shift:40), (scale:1391771264, shift:40), (scale:1390870528, shift:40), (scale:1380812288, shift:40), (scale:1380664832, shift:40), (scale:1387885056, shift:40)], zero_point: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], quantMin: [], quantMax: [], dimension: 0 tfl.pseudo_qconst1
    Input 02 Int32 scale: [(scale:1390494080, shift:48), (scale:1386024832, shift:48), (scale:1368612096, shift:48), (scale:1383288576, shift:48), (scale:1360924032, shift:48), (scale:1396735616, shift:48), (scale:1397640320, shift:48), (scale:1385606144, shift:48), (scale:1392551936, shift:48), (scale:1390251392, shift:48), (scale:1395161856, shift:48), (scale:1383251584, shift:48), (scale:1389071488, shift:48), (scale:1396048768, shift:48), (scale:1396313344, shift:48), (scale:1394423424, shift:48), (scale:1393583872, shift:48), (scale:1396143488, shift:48), (scale:1390353408, shift:48), (scale:1395453184, shift:48), (scale:1375262208, shift:48), (scale:1389420672, shift:48), (scale:1389235072, shift:48), (scale:1396534016, shift:48), (scale:1393501568, shift:48), (scale:1394406656, shift:48), (scale:1392144640, shift:48), (scale:1397129600, shift:48), (scale:1396225408, shift:48), (scale:1386128384, shift:48), (scale:1385980416, shift:48), (scale:1393228416, shift:48)], zero_point: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], quantMin: [], quantMax: [], dimension: 0 tfl.pseudo_qconst
    Output 00 Int8 scale: [(scale:1387409664, shift:37)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 StatefulPartitionedCall_1:0
1 Relu StatefulPartitionedCall_1:0
    Input 00 Int8 scale: [(scale:1387409664, shift:37)], zero_point: [-128], quantMin: [], quantMax: [], dimension: 0 StatefulPartitionedCall_1:0
    Output 00 Int8 scale: [(scale:1387409664, shift:37)], zero_point: [-128], quantMin: [-128], quantMax: [], dimension: 0 StatefulPartitionedCall_1:0

Schedule: 'graph_MAX_BUFFERED'
	0: Operation FullyConnected  - OFM 1, 1, 1, 32
		Kernel: size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0]
		Time index = 0
		Operator Config = OFM Block=[1, 1, 4, 32], IFM Block=[1, 1, 4, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
		IFM Stripe   = [1, 1, 1, 200]
		IFM2 Stripe  = []
		OFM Stripe   = [1, 1, 1, 32]
		Assigned Cascade = 0
		Encoded Weights = 8544 bytes
		Weight buffer = 0 bytes
		Depth slices = [0, 32]
		sub-operations: [ Relu ]
		Estimated Perf: Macs=6400 Cycles=648
		Memory Used: 240 bytes
	Cascades:
################################################################################
Allocation, memory Sram, usage mask: FeatureMap|Staging
Start Time - End Time  : Start Addr -   End Addr: Tensor Size: Memory Usage : Name
         0 -          1:       0x20 -       0xf0:         208:          240 : serving_default_input_1:0
         0 -          3:        0x0 -       0x20:          32:          240 : StatefulPartitionedCall_1:0
Allocation Peak Tensor Size: 240 bytes == 0.234375 KiB
################################################################################
Tensor Allocation for read-only NPU tensors:
Start Time - End Time  : Start Addr -   End Addr: Tensor Size: Memory Usage : Name
         0 -          1:        0x0 -     0x2160:        8544:         8544 : tfl.pseudo_qconst1
Allocation Peak Tensor Size: 8544 bytes == 8.34375 KiB
High level NPU operations:
0 FullyConnected , subOps: Relu, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 4, 32], IFM Block=[1, 1, 4, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
  IFM: serving_default_input_1:0, [1, 1, 1, 200], format: 1, Sram:FeatureMap|Staging, address: 0x20
  OFM: StatefulPartitionedCall_1:0, [1, 1, 1, 32], format: 1, Sram:FeatureMap|Staging, address: 0x0
  Weights: tfl.pseudo_qconst1, 1 ranges, buffering: 0, Dram:ReadOnly, address: 0x0, format: Fast
High level command stream:
0 FullyConnected OFM area [0, 0, 0, 0 - 1, 1, 1, 32], IFM [0, 0, 0, 0 - 1, 1, 1, 200]
Register command stream: 75 words
  Offset: Payload Param Code - Command                        Param, Fields
// FullyConnected , subOps: Relu, size=1,1 stride=1,1, dilation=1,1 padding=[t:0,l:0,b:0,r:0,n:0,f:0] OFM Block=[1, 1, 4, 32], IFM Block=[1, 1, 4, 64], OFM UBlock=[1, 4, 8] Traversal=DepthFirst, AccType=Acc32
0x000000: 40246c8e 0029 4024 - NPU_SET_OFM_SCALE                 41, shift = 41, dbl_rnd = 0, round_mode = ROUND_MODE_OFM_DOUBLE_SYMMETRIC, scale = 1076128910
0x000008:          0001 0105 - NPU_SET_IFM_PRECISION              1, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x00000c:          0001 010f - NPU_SET_IFM_REGION                 1, region = 1
0x000010: 00000020 0000 4000 - NPU_SET_IFM_BASE0                  0, addr = 0x20
0x000018: 00000000 0000 4001 - NPU_SET_IFM_BASE1                  0, addr = 0x0
0x000020: 00000000 0000 4002 - NPU_SET_IFM_BASE2                  0, addr = 0x0
0x000028: 00000000 0000 4003 - NPU_SET_IFM_BASE3                  0, addr = 0x0
0x000030:          0000 010b - NPU_SET_IFM_HEIGHT0_M1             0, height_m1 = 0
0x000034:          0000 010c - NPU_SET_IFM_HEIGHT1_M1             0, height_m1 = 0
0x000038:          0000 010a - NPU_SET_IFM_WIDTH0_M1              0, width_m1 = 0
0x00003c:          00c7 0104 - NPU_SET_IFM_DEPTH_M1             199, depth_m1 = 199
0x000040: 000000c8 0000 4005 - NPU_SET_IFM_STRIDE_Y               0, addr = 0xc8
0x000048: 000000c8 0000 4004 - NPU_SET_IFM_STRIDE_X               0, addr = 0xc8
0x000050: 00000001 0000 4006 - NPU_SET_IFM_STRIDE_C               0, addr = 0x1
0x000058:          ff80 0109 - NPU_SET_IFM_ZERO_POINT         65408, zero_point = 65408
0x00005c:          0000 0107 - NPU_SET_IFM_UPSCALE                0, mode = IFM_UPSCALE_MODE_NONE
0x000060:          0000 0100 - NPU_SET_IFM_PAD_TOP                0, pad = 0
0x000064:          0000 0101 - NPU_SET_IFM_PAD_LEFT               0, pad = 0
0x000068:          0000 0103 - NPU_SET_IFM_PAD_BOTTOM             0, pad = 0
0x00006c:          0000 0102 - NPU_SET_IFM_PAD_RIGHT              0, pad = 0
0x000070:          0001 0114 - NPU_SET_OFM_PRECISION              1, activation_type = ACTIVATION_TYPE_SIGNED, activation_precision = ACTIVATION_PRECISION_B8, activation_format = ACTIVATION_FORMAT_NHWC, scale_mode = OFM_SCALE_MODE_PER_CHANNEL, activation_reverse = ACTIVATION_REVERSE_NONE, activation_transpose = ACTIVATION_TRANSPOSE_HWC, activation_storage = ACTIVATION_STORAGE_TILE2X2
0x000074:          0001 011f - NPU_SET_OFM_REGION                 1, region = 1
0x000078: 00000000 0000 4010 - NPU_SET_OFM_BASE0                  0, addr = 0x0
0x000080: 00000000 0000 4011 - NPU_SET_OFM_BASE1                  0, addr = 0x0
0x000088: 00000000 0000 4012 - NPU_SET_OFM_BASE2                  0, addr = 0x0
0x000090: 00000000 0000 4013 - NPU_SET_OFM_BASE3                  0, addr = 0x0
0x000098:          0000 0112 - NPU_SET_OFM_HEIGHT_M1              0, height_m1 = 0
0x00009c:          0000 0111 - NPU_SET_OFM_WIDTH_M1               0, width_m1 = 0
0x0000a0:          001f 0113 - NPU_SET_OFM_DEPTH_M1              31, depth_m1 = 31
0x0000a4:          0000 011b - NPU_SET_OFM_HEIGHT0_M1             0, height_m1 = 0
0x0000a8:          0000 011c - NPU_SET_OFM_HEIGHT1_M1             0, height_m1 = 0
0x0000ac:          0000 011a - NPU_SET_OFM_WIDTH0_M1              0, width_m1 = 0
0x0000b0: 00000020 0000 4015 - NPU_SET_OFM_STRIDE_Y               0, addr = 0x20
0x0000b8: 00000020 0000 4014 - NPU_SET_OFM_STRIDE_X               0, addr = 0x20
0x0000c0: 00000001 0000 4016 - NPU_SET_OFM_STRIDE_C               0, addr = 0x1
0x0000c8:          ff80 0118 - NPU_SET_OFM_ZERO_POINT         65408, zero_point = 65408
0x0000cc:          0000 0121 - NPU_SET_KERNEL_HEIGHT_M1           0, height_m1 = 0
0x0000d0:          0000 0120 - NPU_SET_KERNEL_WIDTH_M1            0, width_m1 = 0
0x0000d4:          0000 0122 - NPU_SET_KERNEL_STRIDE              0, stride_x_lsb = 0, stride_y_lsb = 0, weight_order = WEIGHT_ORDER_DEPTH_FIRST, dilation_x = KERNEL_DILATION_NONE, dilation_y = KERNEL_DILATION_NONE, decomposition = KERNEL_DECOMPOSITION_D8X8, stride_x_msb = 0, stride_y_msb = 0
0x0000d8:          0001 012e - NPU_SET_WEIGHT_FORMAT              1, weight_format = WEIGHT_FORMAT_FWD, weight_sparsity = WEIGHT_SPARSITY_NONE
0x0000dc:          0000 0128 - NPU_SET_WEIGHT_REGION              0, region = 0
0x0000e0: 00000140 0000 4020 - NPU_SET_WEIGHT_BASE                0, addr = 0x140
0x0000e8: 00002020 0000 4021 - NPU_SET_WEIGHT_LENGTH              0, length = 8224
0x0000f0:          0000 0129 - NPU_SET_SCALE_REGION               0, region = 0
0x0000f4: 00000000 0000 4022 - NPU_SET_SCALE_BASE                 0, addr = 0x0
0x0000fc: 00000140 0000 4023 - NPU_SET_SCALE_LENGTH               0, length = 320
0x000104:          0000 0125 - NPU_SET_ACTIVATION                 0, activation_function = ACTIVATION_FUNCTION_LUT_NONE, table = 0, activation_clip_range = ACTIVATION_CLIP_RANGE_B16
0x000108:          ff80 0126 - NPU_SET_ACTIVATION_MIN         65408, clip_boundary = 65408
0x00010c:          007f 0127 - NPU_SET_ACTIVATION_MAX           127, clip_boundary = 127
0x000110:          0000 0116 - NPU_SET_OFM_BLK_HEIGHT_M1          0, height_m1 = 0
0x000114:          0003 0115 - NPU_SET_OFM_BLK_WIDTH_M1           3, width_m1 = 3
0x000118:          001f 0117 - NPU_SET_OFM_BLK_DEPTH_M1          31, depth_m1 = 31
0x00011c:          0200 0124 - NPU_SET_ACC_FORMAT               512, acc_format = ACC_FORMAT_I32, acc_input = ACC_INPUT_RESET, acc_output = ACC_OUTPUT_ENABLE, microblock = MICROBLOCK_U1X4
0x000120:          0000 012f - NPU_SET_BLOCKDEP                   0, blockdep = 0
0x000124:          0000 0002 - NPU_OP_CONV                        0, weights_ifm2 = 0
0x000128:          ffff 0000 - NPU_OP_STOP                    65535, mask = 65535
Configuration files:
   original = ['../config/bobby.ini']
   used = ['../config/bobby.ini']
System Configuration (AmbiqLP):
   core_clock = 100000000.0
   axi0_port = Sram
   axi1_port = Dram
   Sram_clock_scales = 1.0
   Sram_burst_length = 64
   Sram_read_latency = 9
   Sram_write_latency = 9
   Dram_clock_scales = 1.0
   Dram_burst_length = 128
   Dram_read_latency = 9
   Dram_write_latency = 9
   OnChipFlash_clock_scales = 1.0
   OnChipFlash_burst_length = 1
   OnChipFlash_read_latency = 0
   OnChipFlash_write_latency = 0
   OffChipFlash_clock_scales = 1.0
   OffChipFlash_burst_length = 1
   OffChipFlash_read_latency = 0
   OffChipFlash_write_latency = 0
Memory Mode (Sram_Dram):
   const_mem_area = Axi1
   arena_mem_area = Axi0
   cache_mem_area = Axi0
   arena_cache_size = 1099511627776 from Default
Architecture Settings:
   permanent_storage_mem_area = Dram
   feature_map_storage_mem_area = Sram
   fast_storage_mem_area = Sram

################################################################################
Performance for NPU Grap output/fc_in__200__o_32_relu
Original Operator    NNG Operator         Target Staging Usage  Peak% (Staging)  Op Cycles Network% (cycles)        NPU    SRAM AC    DRAM AC OnFlash AC OffFlash AC  MAC Count Network% (MAC)  Util% (MAC) Name                 
-------------------- -------------------- ------ ------------- ---------------- ---------- ----------------- ---------- ---------- ---------- ---------- ----------- ---------- -------------- ------------ -------------------- 
FullyConnected       FullyConnected       NPU              240           100.00        648            100.00        648         10        534          0           0       6400         100.00         3.86 StatefulPartitionedCall_1:0 
FullyConnected       Relu                 NPU              240           100.00         87             13.43         87          5          0          0           0         32           0.50         0.14 StatefulPartitionedCall_1:0 

Network summary for fc_in__200__o_32_relu
Accelerator configuration               Ethos_U85_256
System configuration                          AmbiqLP
Memory mode                                 Sram_Dram
Accelerator clock                                 100 MHz
Design peak SRAM bandwidth                       2.98 GB/s
Design peak DRAM bandwidth                       1.49 GB/s

Total SRAM used                                  0.23 KiB
Total DRAM used                                  8.34 KiB

CPU operators = 0 (0.0%)
NPU operators = 1 (100.0%)

Average SRAM bandwidth                           0.05 GB/s
Input   SRAM bandwidth                           0.00 MB/batch
Weight  SRAM bandwidth                           0.00 MB/batch
Output  SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth                           0.00 MB/batch
Total   SRAM bandwidth            per input      0.00 MB/inference (batch size 1)

Average DRAM bandwidth                           1.26 GB/s
Input   DRAM bandwidth                           0.00 MB/batch
Weight  DRAM bandwidth                           0.01 MB/batch
Output  DRAM bandwidth                           0.00 MB/batch
Total   DRAM bandwidth                           0.01 MB/batch
Total   DRAM bandwidth            per input      0.01 MB/inference (batch size 1)

Original Weights Size                            6.25 KiB
NPU Encoded Weights Size                         8.03 KiB

Neural network macs                              6400 MACs/batch

Info: The numbers below are internal compiler estimates.
For performance numbers the compiled network should be run on an FVP Model or FPGA.

Network Tops/s                                   0.00 Tops/s

NPU cycles                                        648 cycles/batch
SRAM Access cycles                                 10 cycles/batch
DRAM Access cycles                                534 cycles/batch
On-chip Flash Access cycles                         0 cycles/batch
Off-chip Flash Access cycles                        0 cycles/batch
Total cycles                                      648 cycles/batch

Batch Inference time                 0.01 ms, 154320.99 inferences/s (batch size 1)

